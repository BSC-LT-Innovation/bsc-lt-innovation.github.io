<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Aina RAG (v2)</title>
    <style>
      html, body {
        box-sizing: border-box;
        display: flow-root;
        height: 100%;
        margin: 0;
        padding: 0;
      }
    </style>
    <script type="text/javascript" src="https://cdn.bokeh.org/bokeh/release/bokeh-3.6.2.min.js"></script>
    <script type="text/javascript">
        Bokeh.set_log_level("info");
    </script>
  </head>
  <body>
    <div id="cf04c246-bc1f-4eb6-af65-b2c4ac6a347f" data-root-id="p1458" style="display: contents;"></div>
  
    <script type="application/json" id="f6fce878-4576-4d23-b67e-6a2d068cfa5e">
      {"61dcb71e-5968-43b3-b6fb-e643ee640cbc":{"version":"3.6.2","title":"Bokeh Application","roots":[{"type":"object","name":"Figure","id":"p1458","attributes":{"sizing_mode":"stretch_both","x_range":{"type":"object","name":"DataRange1d","id":"p1459"},"y_range":{"type":"object","name":"DataRange1d","id":"p1460"},"x_scale":{"type":"object","name":"LinearScale","id":"p1468"},"y_scale":{"type":"object","name":"LinearScale","id":"p1469"},"title":{"type":"object","name":"Title","id":"p1461","attributes":{"text":"\nModel: BAAI/bge-m3\nDataset: Aina Challenge (v3)\nDataset size: 65\nEmbeddings shape: (65, 1024)\nDim. reduction: UMAP (params: {'n_neighbors': 4, 'min_dist': 0.1, 'metric': 'cosine', 'random_state': 42})\nTopics: \n- 'General info about BSC models and training': 29 (green)\n- 'Aina Kit': 24 (blue)\n- 'Model Salamandra\u20117B-instruct': 12 (orange)\n","text_font_size":"0.85em","text_font_style":"normal"}},"renderers":[{"type":"object","name":"GlyphRenderer","id":"p1503","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p1494","attributes":{"selected":{"type":"object","name":"Selection","id":"p1495","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p1496"},"data":{"type":"map","entries":[["x",{"type":"ndarray","array":{"type":"bytes","data":"pHwTQeNKEEFKFRFBzioUQWULDEHhXw5BVbkLQfbWFEHI49hA3pTBQAA8x0CBSftAsHX4QI9T+kC+VtlAAl3+QN4J9EBlpcRAq6sIQXJCAkG2AwdBHnADQapfzkAVxt1A7hK4P2Iltj8R9tw/YdiGP48G1z+8ER1AOMLPPwfPfT/Nl4w/+u55P4lF3z/oHro/CJ+HQLElRUDjYZdA0r6SQKWUaUC8h0VAzPxIQFYrd0Ct0qFAImejQGYaTEBJTV9AdX53QHVhUkDurWJAO6qFQH4XAUA515FApAKFQNpVhUCOwFVAYR1nQHqDPkDSdU9Apm1SQGBvYUCP8XJATSFdQCfn0EA="},"shape":[65],"dtype":"float32","order":"little"}],["y",{"type":"ndarray","array":{"type":"bytes","data":"n9PGQGuBukCUabVA4hy8QOAHwEA3BMpAzVXeQCKdvUCbA/lAtGkFQbbUBEGIhQVB41MJQWvaCkFRz/lAfzr/QA5O/kAQygZBpgDMQL672kA5iOBAOlIBQfY7AEHXL+9AScOzQK1SsUAIYbVAPq6dQNAgvkDq5K5ANr2hQNWfvECj/blA+GOfQEGJrEBim59A6U6fQNtEt0AUxq1AxDy5QMFp1kDXWNpAQ6uZQHgpl0C65bRAigSyQBHR5EDQSPlAjCDaQHvL20BlVpRA/33XQGiXo0AQo6ZAi73NQBOm3EDj781ADDjGQGrC1kBVk5xAu3/4QNdk7EAQXM9AmMn9QLU47UA="},"shape":[65],"dtype":"float32","order":"little"}],["topic",{"type":"ndarray","array":["Aina Kit","Aina Kit","Aina Kit","Aina Kit","Aina Kit","Aina Kit","Aina Kit","Aina Kit","Aina Kit","Aina Kit","Aina Kit","Aina Kit","Aina Kit","Aina Kit","Aina Kit","Aina Kit","Aina Kit","Aina Kit","Aina Kit","Aina Kit","Aina Kit","Aina Kit","Aina Kit","Aina Kit","Model Salamandra\u20117B-instruct","Model Salamandra\u20117B-instruct","Model Salamandra\u20117B-instruct","Model Salamandra\u20117B-instruct","Model Salamandra\u20117B-instruct","Model Salamandra\u20117B-instruct","Model Salamandra\u20117B-instruct","Model Salamandra\u20117B-instruct","Model Salamandra\u20117B-instruct","Model Salamandra\u20117B-instruct","Model Salamandra\u20117B-instruct","Model Salamandra\u20117B-instruct","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training","General info about BSC models and training"],"shape":[65],"dtype":"object","order":"little"}],["question",{"type":"ndarray","array":["Aina Kit\nCobertura d'idiomes de l'Aina Kit\nLa majoria dels models de l'Aina Kit s\u00f3n multiling\u00fces amb un suport s\u00f2lid per al catal\u00e0, l'espanyol i l'angl\u00e8s. En molts casos, tamb\u00e9 cobreixen altres lleng\u00fces europees, cosa que facilita fluxos multiling\u00fces (p. ex., RAG i traducci\u00f3 assistida). Tot i aix\u00ed, per a tasques espec\u00edfiques de catal\u00e0, hi ha models i recursos amb suport ple a causa de l'enfocament del projecte AINA en aquest idioma.","Aina Kit\nFam\u00edlies de models de text a l'Aina Kit\nL'Aina Kit ofereix diverses fam\u00edlies de models amb casos d'\u00fas diferents: models fundacionals (per a comprensi\u00f3/generaci\u00f3 generalista), models instru\u00efts (optimitzats per a conversa i seguiment d'instruccions) i models especialitzats (NER, classificaci\u00f3, QA o embeddings). Per a NLP cl\u00e0ssic, tamb\u00e9 hi ha models de la fam\u00edlia AINA/NLP (com marian per a traducci\u00f3, roberta/deberta per a classificaci\u00f3 i NER, i models d'embeddings). Aquests \u00faltims destaquen en classificaci\u00f3, NER, QA i creaci\u00f3 d'embeddings densos.","Aina Kit\nTipus de model: fundacional, instru\u00eft i adaptat\nUn model fundacional (p. ex., salamandra-7b o ALIA-40B) apr\u00e8n de grans quantitats de dades i serveix com a base generalista. Un model instru\u00eft parteix d'aquest i s'afegeix fine-tuning amb converses i instruccions perqu\u00e8 respongui millor en di\u00e0leg. Un model adaptat es focalitza en un domini concret (jur\u00eddic, m\u00e8dic, etc.) mitjan\u00e7ant fine-tuning amb dades del domini, aconseguint millor rendiment en aquest \u00e0mbit a canvi de menor versatilitat general.","Aina Kit\nModels recomanats per a NER en catal\u00e0\nPer al reconeixement d'entitats en catal\u00e0, l'Aina Kit recomana models basats en transformers adaptats a NER. Entre els destacats hi ha variants entrenades espec\u00edficament per a NER en catal\u00e0 (p. ex., models basats en roberta/deberta) i un model fort basat en deberta-v3-large entrenat amb el conjunt de dades CEIL. Les fitxes de cada model inclouen m\u00e8triques, cobertures i pautes d'\u00fas.","Aina Kit\nEmbeddings de paraules disponibles\nL'Aina Kit ofereix embeddings CBOW en catal\u00e0 entrenats amb aproximadament 4.300 milions de paraules; tamb\u00e9 hi ha embeddings contextuals (sent\u00e8ncia/paraula) basats en transformers. Aquests recursos capten relacions sem\u00e0ntiques i s\u00f3n \u00fatils per a recuperaci\u00f3, classificaci\u00f3, cerca sem\u00e0ntica, clustering i com a base per a pipelines RAG.","Aina Kit\nQu\u00e8 \u00e9s el RAG i qu\u00e8 ofereix l'Aina Kit\nRAG (Retrieval-Augmented Generation) connecta un LLM a una base de coneixement: primer recupera passatges rellevants i despr\u00e9s el model genera la resposta amb aquest context. L'Aina Kit inclou dades, scripts i bones pr\u00e0ctiques per a RAG multiling\u00fce. Tamb\u00e9 ofereix un dataset amb exemples de preguntes, contextos i respostes en catal\u00e0, espanyol i angl\u00e8s, dissenyat per entrenar i avaluar sistemes RAG.","Aina Kit\nModels de veu: TTS i ASR\nL'Aina Kit inclou models de veu per a dues tasques fonamentals: ASR (Automatic Speech Recognition) per transcriure \u00e0udio a text i TTS (Text-To-Speech) per sintetitzar parla a partir de text. S'ofereixen instruccions d'\u00fas, requisits de maquinari i refer\u00e8ncies a la documentaci\u00f3 i disponibilitat a la p\u00e0gina de models de veu del projecte.","Aina Kit\nModels de text vs. models de traducci\u00f3 autom\u00e0tica\nEls models de text de l'Aina Kit s\u00f3n de prop\u00f2sit general i funcionen b\u00e9 en m\u00faltiples tasques, per\u00f2 no substitueixen els models de traducci\u00f3 especialitzats quan es requereix la m\u00e0xima precisi\u00f3 en parells ling\u00fc\u00edstics concrets. Per a traducci\u00f3 de producci\u00f3, s'aconsella avaluar models dedicats (marian, nllb, etc.) i integrar-los quan la qualitat i la consist\u00e8ncia siguin priorit\u00e0ries.","Aina Kit\nDisponibilitat: API i desplegament\nEls models estan disponibles via API i com a desplegaments autogestionats. L'Aina Kit documenta endpoints, esquemes d'autenticaci\u00f3 i bones pr\u00e0ctiques de consum en entorns de prova i producci\u00f3, incloent imatges de contenidors, requisits de recursos i guies d'integraci\u00f3.","Aina Kit\nCom comen\u00e7ar a usar els models localment\nPer comen\u00e7ar localment, instal\u00b7leu depend\u00e8ncies (Python, llibreries de ML i, si escau, acceleradors), descarregueu el model (p. ex., via Hugging Face) i executeu exemples d'infer\u00e8ncia b\u00e0sica per validar l'entorn. La documentaci\u00f3 inclou snippets i consells per a GPU/CPU, mem\u00f2ria i configuracions de context.","Aina Kit\nOpcions de desplegament en producci\u00f3\nEl desplegament en producci\u00f3 pot fer-se amb frameworks d'infer\u00e8ncia optimitzats (vLLM, TGI, etc.), escalat horitzontal, autoscaling, i monitoratge (lat\u00e8ncia, taxa d'errors, \u00fas de mem\u00f2ria i GPU). Tamb\u00e9 es recomanen pr\u00e0ctiques de logging, tracing, cache de resultats i blue/green deploys per actualitzacions sense interrupcions.","Aina Kit\nFine-tuning i llic\u00e8ncies dels models\nEl fine-tuning est\u00e0 subjecte a les llic\u00e8ncies dels models i de les dades. Abans d'entrenar i distribuir pesos ajustats, cal revisar condicions d'\u00fas, atribuci\u00f3 i compatibilitat de llic\u00e8ncies. Alguns models permeten \u00fas comercial sense restriccions fortes; d'altres exigeixen requisits concrets de redistribuci\u00f3 o \u00fas.","Aina Kit\nQuan conv\u00e9 fer fine-tuning\nConvindr\u00e0 fer fine-tuning quan calgui adaptar el model a un domini o estil, quan l'avaluaci\u00f3 interna mostri bretxes espec\u00edfiques o quan hi hagi dades propiet\u00e0ries que aportin millora clara. Si el model base ja compleix els requisits, potser n'hi ha prou amb prompt engineering o RAG, sense necessitat d'ajustar pesos.","Aina Kit\nCom fer fine-tuning a la pr\u00e0ctica\nA la pr\u00e0ctica, el fine-tuning es realitza amb SFT/QLoRA o full fine-tuning en funci\u00f3 de recursos i objectius. Es defineixen formats de dades, plantilles de conversa i m\u00e8triques de validaci\u00f3. Es recomana fer validacions peri\u00f2diques amb conjunts de prova representatius i controlar sobreajustament, drift i regressions.","Aina Kit\nFunction calling i \u00fas d'eines\nEl function calling permet que el model invoqui eines externes (APIs, bases de dades, c\u00e0lcul) mitjan\u00e7ant sortides estructurades (JSON). Aquest enfocament millora la fiabilitat i tra\u00e7abilitat en fluxos composables (p. ex., agents que planifiquen i executen passos), i redueix al\u00b7lucinacions quan la informaci\u00f3 prov\u00e9 d'eines verificables.","Aina Kit\nLlic\u00e8ncies dels models a l'Aina Kit\nLes llic\u00e8ncies dels models a l'Aina Kit inclouen variants permissives (com Apache 2.0) i d'altres amb restriccions. Cada fitxa de model detalla termes d'\u00fas, atribuci\u00f3, limitacions de responsabilitat i compatibilitats amb distribuci\u00f3 de pesos ajustats. Abans d'integrar en producte, reviseu la llic\u00e8ncia espec\u00edfica.","Aina Kit\nImplicacions pr\u00e0ctiques d'Apache 2.0\nLes implicacions d'Apache 2.0 inclouen \u00fas comercial, modificaci\u00f3 i distribuci\u00f3 dels binaris i codi, sempre mantenint els avisos de copyright i la llic\u00e8ncia. No ofereix garanties; s'aconsella revisar obligacions de marca, patents i atribuci\u00f3, especialment en redistribuci\u00f3 de versions modificades.","Aina Kit\nRequisits de maquinari per executar models\nEls requisits de maquinari depenen de la mida del model i de la longitud de context. Per a prototipat, pot bastar una GPU de gamma mitjana; per a servint de baixa lat\u00e8ncia amb contexts llargs i batch alt, calen GPU com A100/H100 i optimitzacions d'infer\u00e8ncia (cuBLAS, quantitzaci\u00f3, KV caching).","Aina Kit\nTipus de datasets disponibles a l'Aina Kit\nEls tipus de datasets inclouen corpus multiling\u00fces, conversacionals, t\u00e8cnics i espec\u00edfics de domini, amb metadades i llic\u00e8ncies documentades. Hi ha col\u00b7leccions per a ajust d'instruccions, classificaci\u00f3, NER, QA, i conjunts per a RAG amb preguntes, passatges i respostes alineades.","Aina Kit\nEines per al subministrament de dades de text\nPer a text, s'ofereixen eines de neteja, deduplicaci\u00f3, normalitzaci\u00f3 i segmentaci\u00f3, aix\u00ed com scripts per generar splits consistents i verificables. L'objectiu \u00e9s facilitar la preparaci\u00f3, la tra\u00e7abilitat i la reprodu\u00efbilitat dels conjunts de dades.","Aina Kit\nEines per al subministrament de dades de veu\nPer a veu, hi ha eines de transcripci\u00f3, alineaci\u00f3, normalitzaci\u00f3 i validaci\u00f3 de qualitat, a m\u00e9s de guies per alinear transcripcions amb \u00e0udio i preparar datasets per a ASR/TTS de manera fiable.","Aina Kit\nLlic\u00e8ncies dels datasets\nLes llic\u00e8ncies dels datasets varien (CC-BY, CC0, propiet\u00e0ries, etc.). Abans d'entrenar o redistribuir, conv\u00e9 comprovar compatibilitats de llic\u00e8ncia, requisits d'atribuci\u00f3 i restriccions d'\u00fas comercial o derivats.","Aina Kit\nDemostradors i acc\u00e9s r\u00e0pid\nHi ha demostradors i punts d'acc\u00e9s r\u00e0pid per provar funcionalitats sense desplegaments complexos. Inclouen interf\u00edcies web, notebooks i endpoints m\u00ednims per fer proves d'infer\u00e8ncia i integrar fluxos b\u00e0sics.","Aina Kit\nComunitat i com mantenir-se al dia\nLa comunitat disposa de canals per mantenir-se al dia (repos, f\u00f2rums, canals socials i butlletins). S'hi comparteixen novetats, fulls de ruta, models nous i bones pr\u00e0ctiques d'\u00fas, a m\u00e9s de suport col\u00b7laboratiu.","Model Salamandra\u20117B-instruct\nSalamandra: descripci\u00f3 general i llic\u00e8ncia\nSalamandra \u00e9s una fam\u00edlia de models oberts amb variants fundacionals i instru\u00efdes. La llic\u00e8ncia permet recerca i \u00fas responsable, i les fitxes inclouen detalls de versions, context m\u00e0xim, compatibilitats i consideracions pr\u00e0ctiques d'\u00fas.","Model Salamandra\u20117B-instruct\nPreentrenament i abast de dades a Salamandra\nEl preentrenament abasta dades multiling\u00fces i dominis variats per aconseguir cobertura robusta en catal\u00e0, espanyol i angl\u00e8s, entre d'altres. S'han aplicat filtres i criteris de qualitat per millorar la senyal i reduir soroll o duplicats.","Model Salamandra\u20117B-instruct\nArquitectura i especificacions de Salamandra-7B-Instruct\nSalamandra-7B-Instruct empra una arquitectura moderna amb optimitzacions d'efici\u00e8ncia, atenci\u00f3 i mem\u00f2ria. Les especificacions inclouen mida del vocabulari, longitud de context i par\u00e0metres orientats a qualitat i rendiment en infer\u00e8ncia.","Model Salamandra\u20117B-instruct\nUsos previstos i limitacions\nEls usos previstos inclouen assistents, resum, RAG i extracci\u00f3 d'informaci\u00f3. Les limitacions conegudes inclouen errors factuals sota manca de context, biaixos heretats del preentrenament i dificultats en c\u00e0lcul num\u00e8ric estricte o raonament multi-pas sense suport extern.","Model Salamandra\u20117B-instruct\nInfraestructura i framework d'entrenament\nLa infraestructura d'entrenament empra frameworks eficients i pipelines reproductibles a escala, amb registre de m\u00e8triques, checkpoints i validacions. Es detallen requisits de maquinari, paral\u00b7lelisme i pr\u00e0ctiques per a entrenaments estables.","Model Salamandra\u20117B-instruct\nPlantilla conversacional (ChatML) per a infer\u00e8ncia\nLes variants instru\u00efdes usen una plantilla de xat tipus ChatML: cada torn inclou rols (usuari/assistent) i delimitadors coherents. Aquesta estructura millora l'estabilitat i la qualitat de generaci\u00f3, especialment en di\u00e0legs llargs i tasques de seguiment d'instruccions.","Model Salamandra\u20117B-instruct\nDades d'ajust d'instruccions\nLa variant instru\u00efda es va afinar amb una col\u00b7lecci\u00f3 d'aproximadament centenars de milers d'exemples d'instruccions i di\u00e0legs, incloent conjunts com oasst-ca, oasst2, open-orca, rag-multilingual i tower-blocks, entre d'altres, amb curaci\u00f3 i filtratge per qualitat.","Model Salamandra\u20117B-instruct\nAvaluaci\u00f3 i consideracions de replicaci\u00f3\nL'avaluaci\u00f3 amb Language Model Evaluation Harness abasta m\u00faltiples bancs de proves i tasques; es reporten m\u00e8triques de precisi\u00f3, F1 i altres indicadors rellevants. Tot i aix\u00f2, els benchmarks no capten tota l'amplitud de capacitats del model i s'han de complementar amb avaluacions espec\u00edfiques del cas d'\u00fas.","Model Salamandra\u20117B-instruct\nAvaluaci\u00f3 amb LLM-as-a-judge\nA m\u00e9s dels benchmarks de refer\u00e8ncia, s'ha emprat Prometheus-2 com a LLM-as-a-judge per qualificar la qualitat de respostes i la seva coher\u00e8ncia, amb l'objectiu d'avaluar atributs intr\u00ednsecs m\u00e9s enll\u00e0 de m\u00e8triques objectives cl\u00e0ssiques.","Model Salamandra\u20117B-instruct\nConsideracions \u00e8tiques i limitacions detectades\nLes proves de biaix social amb BBQ i Regard mostren bons resultats relatius, per\u00f2 persisteixen limitacions inherents als conjunts de dades i al preentrenament. S'aconsella incorporar gu\u00e0rdies de seguretat, filtres i ajustos addicionals segons l'aplicaci\u00f3 final.","Model Salamandra\u20117B-instruct\nCom usar Salamandra-7B-Instruct a la pr\u00e0ctica\nPer a infer\u00e8ncia, carregueu el model 'BSC-LT/salamandra-7b-instruct' i seguiu la plantilla de conversa indicada (ChatML). Per a RAG, incloeu els fragments recuperats com a context al prompt, i gestioneu longitud de context, truncament i cites si cal.","Model Salamandra\u20117B-instruct\nSalamandra i RAG\nLes variants instru\u00efdes de Salamandra funcionen b\u00e9 en pipelines RAG multiling\u00fces. Amb dades en ca/es/en, \u00e9s \u00fatil per entrenar i avaluar aquest tipus de fluxos, validant tant la recuperaci\u00f3 com la generaci\u00f3 condicional al context.","General info about BSC models and training\nQuickstart: Inference with Transformers (Python)\nA quick way to run BSC models is to use the Hugging Face Transformers stack. Pick the exact model repository, load the tokenizer and the model, and generate a response with a sensible limit for new tokens. \nKey points: ensure your environment matches the model\u2019s requirements (CUDA/PyTorch versions), prefer half precision on GPUs to reduce memory, and respect the model\u2019s documented chat template and special tokens. \nIf you see truncated outputs or odd behavior, check the maximum context length and the presence of stop sequences. For CPU-only machines, expect slower generation and consider smaller or quantized variants.","General info about BSC models and training\nPrompt Template (chat-style)\nInstruction-tuned variants from BSC document a chat-style template (ChatML) in their model cards. Keep a short, stable system message, a clear user request, and let the assistant answer directly. \nBase variants expect plain text prompts (no roles) unless you build the format yourself. When switching between base and instruct, review the tokenizer\u2019s special tokens and the documented chat template to avoid format drift.","General info about BSC models and training\nFinetuning with QLoRA (PEFT) \u2013 Single GPU\nQLoRA attaches lightweight adapters to a frozen base model, enabling finetuning on a single GPU with limited VRAM. It is ideal when you want to specialize a model for a niche domain without touching all the original weights.\nTypical practice: choose modest adapter ranks, train for a few epochs, and monitor both loss and task\u2011specific metrics. Keep your dataset formatted consistently with the model\u2019s chat style. After training, you can publish just the adapters or merge them for simpler deployment. QLoRA usually maintains base model quality while adding your domain\u2011specific behavior.","General info about BSC models and training\nFull Finetuning (FSDP/DeepSpeed) \u2013 Multi\u2011GPU\nFull finetuning updates all parameters and is best when you need stronger domain shifts or architectural features not well captured by adapters. It requires multi\u2011GPU sharding and careful memory management (e.g., FSDP or ZeRO\u20113), plus gradient checkpointing for long sequences.\nUse conservative learning rates, frequent evaluation, and robust logging. Watch for catastrophic forgetting: mix in a small portion of general data or add regularization to preserve core skills. Expect higher cost and longer training times than adapter\u2011based methods.","General info about BSC models and training\nData Formatting for Instruction Tuning\nInstruction tuning works best when examples mirror how the model will be prompted in production. Represent each training case as a small conversation with roles (system, user, assistant). Keep the instruction concise, the user request unambiguous, and the assistant reply directly tied to the task.\nIf your application needs structured outputs, make that explicit and keep the format constant across the dataset. Remove noise, unify punctuation and whitespace, and avoid leaking evaluation answers into training.","General info about BSC models and training\nEvaluation: Basic Accuracy &amp; Format Compliance\nEvaluate on a held\u2011out set that reflects real usage. For open\u2011ended answers, measure exact or fuzzy match against references and sample a subset for human review. For structured outputs, track parse success and field\u2011level correctness.\nOperational metrics matter: time\u2011to\u2011first\u2011token, tokens per second, and memory use under concurrency. Compare to a trusted baseline and stop training once quality plateaus or regressions appear.","General info about BSC models and training\nServing with vLLM\nvLLM is a high\u2011throughput inference server that uses PagedAttention to handle many requests efficiently. It offers an OpenAI\u2011compatible API, strong streaming support, and good long\u2011context performance on modern GPUs.\nIt shines in scenarios with mixed request sizes and high concurrency. Validate that your chosen quantization and tensor parallel settings match the hardware, and confirm that stop sequences and chat templates behave as expected.","General info about BSC models and training\nServing with Text Generation Inference (TGI)\nTGI is a production\u2011oriented server maintained by Hugging Face. It provides mature endpoints, streaming, token usage reporting, and integrations with the Hub and Inference Endpoints.\nIt is a solid default for teams already in the HF ecosystem. Choose appropriate max token limits, adjust batching for latency vs. throughput, and verify tokenizer compatibility when switching between checkpoints or quantized variants.","General info about BSC models and training\nQuantization Options\nQuantization compresses a model by representing weights in lower precision integers (e.g., INT8/INT4) with learned scale/zero-point instead of FP16/BF16. This cuts VRAM/RAM and memory bandwidth, improving throughput and enabling larger models or batches on the same hardware. There are different options: weight-only schemes (e.g., GPTQ, AWQ, FP8 weights) keep activations in FP16/BF16; activation-aware schemes also quantize activations (and occasionally the KV cache), saving more memory but risking higher quality loss. Methods trade accuracy for efficiency in different ways (GPTQ minimizes per-channel error after calibration; AWQ protects \u201coutlier\u201d channels; bitsandbytes 4-bit is commonly used for adapter training like QLoRA). Expect small drops in perplexity and possible degradations in reasoning, long-context behavior, or strict JSON formatting. Always validate on your task (exactness, format compliance, latency, tokens/s, memory), prefer officially published quantized checkpoints when available, and treat community conversions as approximate until tested in your serving stack (vLLM/TGI/llama.cpp).","General info about BSC models and training\nMemory &amp; Hardware Sizing\nAs a rough guide for FP16 inference: 2B models fit in 4\u20138 GB VRAM; 7B in ~14\u201320 GB; 13B in ~24\u201328 GB; and 40B often requires 70+ GB with tensor parallelism. Quantization lowers these requirements but can impact quality.\nThroughput is determined by batch size, context length, and concurrency; latency depends on model size, KV\u2011cache strategy, and I/O. Always right\u2011size the model to the task instead of scaling up by default.","General info about BSC models and training\nReproducibility &amp; Experiment Tracking\nReproducible experiments require fixed seeds, versioned code, and pinned dependencies. Save the tokenizer and configuration alongside checkpoints to prevent drift.\nUse a tracker (e.g., MLflow or W&amp;B) for metrics, hyperparameters, and artifacts. Store evaluation reports and data snapshots so future runs can be compared fairly.","General info about BSC models and training\nLicensing &amp; Responsible Use\nReview the model card license and intended\u2011use sections before deployment. Some checkpoints are research\u2011only or restrict commercial usage. Document known limitations and add safeguards for abuse, privacy, and bias.\nIn production, implement rate limits, input validation, and output filtering where appropriate. Be transparent with users about capabilities and boundaries.","General info about BSC models and training\nDatasets: Curation Tips for High\u2011Quality SFT\nPrioritize quality and task alignment over raw volume. De\u2011duplicate aggressively, normalize formatting, and remove low\u2011signal examples. Include edge cases and negative examples so the model learns when to say \u201cI don\u2019t know.\u201d\nFor multilingual work (ES/CAT/PT/EN), balance languages and domains to avoid regressions. Keep a clean validation split to measure real progress.","General info about BSC models and training\nStructured Outputs (JSON\u2011only)\nIf your system expects strict JSON, teach the model with consistent instructions and clean examples. Keep keys stable, avoid prose around the JSON, and use schemas during evaluation.\nIn production, validate every response; if parsing fails, request a corrected answer using the same schema. This simple loop dramatically improves reliability.","General info about BSC models and training\nStreaming &amp; Token\u2011by\u2011Token UX\nStreaming improves perceived responsiveness by delivering tokens as they are generated. It is especially valuable for long answers or slow backends. \nMake sure the UI handles partial sentences gracefully and that you set stop sequences to prevent trailing boilerplate. Log both time\u2011to\u2011first\u2011token and end\u2011to\u2011end latency.","General info about BSC models and training\nRAG Integration Notes\nIn RAG systems, retrieval quality is as important as the model. Chunk documents into semantically coherent pieces, store citations, and prefer small, high\u2011relevance context windows over large, noisy ones.\nAdd lightweight re\u2011ranking, instruct the model to quote sources, and score answers for faithfulness. Cache frequent queries and refresh the index on a schedule that matches your content updates.","General info about BSC models and training\nContext windows and tokenizer notes\nAs a rule of thumb, Salamandra 2B/7B variants expose an 8,192\u2011token context, while ALIA\u201140B (base) exposes 32,768 tokens. All use a 256k\u2011token vocabulary and RoPE position embeddings. Before migration between variants, confirm the documented chat template (for instruct) and special tokens to keep prompts and stop sequences consistent.","General info about BSC models and training\nTraining stack and infrastructure\nBSC model cards document pre\u2011training with NVIDIA NeMo in large distributed setups and report that instruction\u2011tuned variants were produced with FastChat. Training took place on MareNostrum 5 (EuroHPC) with Hopper GPUs. More details can be found on the model cards of the specific models on Hugging Face.","General info about BSC models and training\nWhen to add a reranker\nIf first\u2011stage retrieval brings many near\u2011ties or domain jargon, a cross\u2011encoder reranker often boosts precision significantly. Use it on the top\u2011k retrieved chunks (e.g., 50\u21925) to improve faithfulness without inflating context size. Rerankers pay off especially for technical queries in Spanish/Catalan where lexical overlap is high.","General info about BSC models and training\nLong\u2011context prompting strategies\nExploit long contexts deliberately: keep instructions stable, place the most relevant citations near the end of the prompt to counter recency effects, and avoid dumping large, noisy contexts. For ALIA\u201140B\u2019s longer window, prefer fewer but higher\u2011quality chunks and ensure citations include enough surrounding text for verification.","General info about BSC models and training\nFunction calling &amp; tools\nBSC model currently focus on text generation, and are not specialized for function calling. You can still implement tool use via fine-tuning, prompting, or middleware that parses structured outputs. Define strict output formats (e.g., JSON) and validate them server\u2011side before executing any action.","General info about BSC models and training\nChoosing base vs. instruct for finetuning\nPick base when you need full control over style and behavior or are training for non\u2011conversational tasks. Choose instruct when your target is assistant\u2011like dialogue and your dataset already follows a chat format. To avoid catastrophic forgetting, keep a small mix of general data or apply regularization during training.","General info about BSC models and training\nEvaluation playbook for ES/CAT tasks\nCover both content and format: exact/fuzzy match for open answers, strict JSON parsing for structured outputs, and field\u2011level scoring for extractions. Include adversarial cases (typos, ambiguous dates) in Spanish and Catalan and track latency/tokens\u2011per\u2011second so model and server changes are comparable over time.","General info about BSC models and training\nServing: throughput vs. latency trade\u2011offs\nvLLM and TGI are common choices for serving BSC models. Tune batching and concurrency for your workload: batch more for cost and throughput; limit batch size and stream tokens for responsiveness. Validate stop sequences and template handling to prevent assistant preambles and trailing artifacts.","General info about BSC models and training\nOperational safeguards\nAdd guardrails around inputs and outputs: rate limits, timeouts, length checks, and schema validation for structured responses. Log minimally with privacy in mind, and add simple automatic checks for unsafe or out\u2011of\u2011policy requests before forwarding them to downstream systems.","General info about BSC models and training\nData governance for instruction tuning\nMaintain a clear data card for your finetuning set: provenance, licenses, languages, and known limitations. De\u2011duplicate aggressively, normalize formatting, and include negative and edge\u2011case examples. Version your dataset and pin training runs to exact commits to keep experiments auditable.","General info about BSC models and training\nKnown limitations &amp; failure modes\nInstruction\u2011tuned variants are not RLHF\u2011aligned and may produce unsafe or biased outputs; base variants are not conversational by default. Expect occasional format drift, arithmetic mistakes, and sensitivity to prompt phrasing, especially in multilingual settings or with very long contexts.","General info about BSC models and training\nResponsible use &amp; privacy notes\nReview licenses and intended use before deployment. In production, mask personally identifiable information in logs, restrict retention, and be transparent with users about capabilities and boundaries. For high\u2011stakes applications, add domain\u2011specific safety testing and human oversight.","General info about BSC models and training\nSupport &amp; community channels\nFor official questions or issues, check the model card\u2019s contact information and open an issue on the repository. On the Aina Kit official website (https://langtech-bsc.gitbook.io/aina-kit), you can also find a FAQ page and a link to our Discord channel, where the BSC staff and community will be able to help you with specific questions."],"shape":[65],"dtype":"object","order":"little"}]]}}},"view":{"type":"object","name":"CDSView","id":"p1504","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p1505"}}},"glyph":{"type":"object","name":"Scatter","id":"p1500","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":15},"line_color":{"type":"value","value":"white"},"line_alpha":{"type":"value","value":0.96},"fill_color":{"type":"field","field":"topic","transform":{"type":"object","name":"CategoricalColorMapper","id":"p1457","attributes":{"palette":["#1f77b4","#ff7f0e","#2ca02c"],"factors":{"type":"ndarray","array":["Aina Kit","Model Salamandra\u20117B-instruct","General info about BSC models and training"],"shape":[3],"dtype":"object","order":"little"}}}},"fill_alpha":{"type":"value","value":0.96},"hatch_color":{"type":"field","field":"topic","transform":{"id":"p1457"}},"hatch_alpha":{"type":"value","value":0.96}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p1501","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":15},"line_color":{"type":"value","value":"white"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"field","field":"topic","transform":{"id":"p1457"}},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"field","field":"topic","transform":{"id":"p1457"}},"hatch_alpha":{"type":"value","value":0.1}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p1502","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":15},"line_color":{"type":"value","value":"white"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"field","field":"topic","transform":{"id":"p1457"}},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"field","field":"topic","transform":{"id":"p1457"}},"hatch_alpha":{"type":"value","value":0.2}}}}},{"type":"object","name":"GlyphRenderer","id":"p1515","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p1506","attributes":{"selected":{"type":"object","name":"Selection","id":"p1507","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p1508"},"data":{"type":"map","entries":[["x",{"type":"ndarray","array":{"type":"bytes","data":""},"shape":[0],"dtype":"float32","order":"little"}],["y",{"type":"ndarray","array":{"type":"bytes","data":""},"shape":[0],"dtype":"float32","order":"little"}],["topic",[]],["question",{"type":"ndarray","array":[],"shape":[0],"dtype":"object","order":"little"}]]}}},"view":{"type":"object","name":"CDSView","id":"p1516","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p1517"}}},"glyph":{"type":"object","name":"Scatter","id":"p1512","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":20},"line_color":{"type":"value","value":"#aaaaaa"},"fill_color":{"type":"value","value":"#aaaaaa"},"hatch_color":{"type":"value","value":"#aaaaaa"},"marker":{"type":"value","value":"x"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p1513","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":20},"line_color":{"type":"value","value":"#aaaaaa"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"#aaaaaa"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"#aaaaaa"},"hatch_alpha":{"type":"value","value":0.1},"marker":{"type":"value","value":"x"}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p1514","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":20},"line_color":{"type":"value","value":"#aaaaaa"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"#aaaaaa"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"#aaaaaa"},"hatch_alpha":{"type":"value","value":0.2},"marker":{"type":"value","value":"x"}}}}},{"type":"object","name":"GlyphRenderer","id":"p1524","attributes":{"data_source":{"type":"object","name":"ColumnDataSource","id":"p1518","attributes":{"selected":{"type":"object","name":"Selection","id":"p1519","attributes":{"indices":[],"line_indices":[]}},"selection_policy":{"type":"object","name":"UnionRenderers","id":"p1520"},"data":{"type":"map","entries":[["x",{"type":"ndarray","array":{"type":"bytes","data":""},"shape":[0],"dtype":"float32","order":"little"}],["y",{"type":"ndarray","array":{"type":"bytes","data":"n9PGQGuBukCUabVA4hy8QOAHwEA3BMpAzVXeQCKdvUCbA/lAtGkFQbbUBEGIhQVB41MJQWvaCkFRz/lAfzr/QA5O/kAQygZBpgDMQL672kA5iOBAOlIBQfY7AEHXL+9AScOzQK1SsUAIYbVAPq6dQNAgvkDq5K5ANr2hQNWfvECj/blA+GOfQEGJrEBim59A6U6fQNtEt0AUxq1AxDy5QMFp1kDXWNpAQ6uZQHgpl0C65bRAigSyQBHR5EDQSPlAjCDaQHvL20BlVpRA/33XQGiXo0AQo6ZAi73NQBOm3EDj781ADDjGQGrC1kBVk5xAu3/4QNdk7EAQXM9AmMn9QLU47UA="},"shape":[65],"dtype":"float32","order":"little"}]]}}},"view":{"type":"object","name":"CDSView","id":"p1525","attributes":{"filter":{"type":"object","name":"AllIndices","id":"p1526"}}},"glyph":{"type":"object","name":"Scatter","id":"p1521","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":20},"line_color":{"type":"value","value":"#fa9fb5"},"fill_color":{"type":"value","value":"#fa9fb5"},"hatch_color":{"type":"value","value":"#fa9fb5"},"marker":{"type":"value","value":"x"}}},"nonselection_glyph":{"type":"object","name":"Scatter","id":"p1522","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":20},"line_color":{"type":"value","value":"#fa9fb5"},"line_alpha":{"type":"value","value":0.1},"fill_color":{"type":"value","value":"#fa9fb5"},"fill_alpha":{"type":"value","value":0.1},"hatch_color":{"type":"value","value":"#fa9fb5"},"hatch_alpha":{"type":"value","value":0.1},"marker":{"type":"value","value":"x"}}},"muted_glyph":{"type":"object","name":"Scatter","id":"p1523","attributes":{"x":{"type":"field","field":"x"},"y":{"type":"field","field":"y"},"size":{"type":"value","value":20},"line_color":{"type":"value","value":"#fa9fb5"},"line_alpha":{"type":"value","value":0.2},"fill_color":{"type":"value","value":"#fa9fb5"},"fill_alpha":{"type":"value","value":0.2},"hatch_color":{"type":"value","value":"#fa9fb5"},"hatch_alpha":{"type":"value","value":0.2},"marker":{"type":"value","value":"x"}}}}}],"toolbar":{"type":"object","name":"Toolbar","id":"p1467","attributes":{"tools":[{"type":"object","name":"PanTool","id":"p1480"},{"type":"object","name":"WheelZoomTool","id":"p1481","attributes":{"renderers":"auto"}},{"type":"object","name":"BoxZoomTool","id":"p1482","attributes":{"overlay":{"type":"object","name":"BoxAnnotation","id":"p1483","attributes":{"syncable":false,"line_color":"black","line_alpha":1.0,"line_width":2,"line_dash":[4,4],"fill_color":"lightgrey","fill_alpha":0.5,"level":"overlay","visible":false,"left":{"type":"number","value":"nan"},"right":{"type":"number","value":"nan"},"top":{"type":"number","value":"nan"},"bottom":{"type":"number","value":"nan"},"left_units":"canvas","right_units":"canvas","top_units":"canvas","bottom_units":"canvas","handles":{"type":"object","name":"BoxInteractionHandles","id":"p1489","attributes":{"all":{"type":"object","name":"AreaVisuals","id":"p1488","attributes":{"fill_color":"white","hover_fill_color":"lightgray"}}}}}}}},{"type":"object","name":"SaveTool","id":"p1490"},{"type":"object","name":"ResetTool","id":"p1491"},{"type":"object","name":"HelpTool","id":"p1492"},{"type":"object","name":"HoverTool","id":"p1493","attributes":{"renderers":"auto","tooltips":"\n&lt;!--&lt;div style=\"padding: 5px; border: 1px solid #fff157; background-color: #fffff4; max-width: 650px\"&gt;--&gt;\n&lt;div style=\"padding: 15px; border: 0.5px solid; background-color: #ffffff; max-width: 650px\"&gt;\n    \n    &lt;div&gt;\n        &lt;span style=\"font-size: 15px; font-weight: bold;\"&gt;VDB index: $index&lt;/span&gt;\n        &lt;br&gt;\n        &lt;span style=\"font-size: 15px; font-weight: bold;\"&gt;Topic: @topic&lt;/span&gt;\n        &lt;hr&gt;\n    &lt;/div&gt;\n\n    &lt;div&gt;&lt;span style=\"font-size: 15px;\"&gt;@question&lt;/span&gt;&lt;/div&gt;\n    &lt;div&gt;&lt;span style=\"font-size: 10px; color: #69f;\"&gt;($x, $y)&lt;/span&gt;&lt;/div&gt;\n&lt;/div&gt;\n"}}],"active_scroll":{"id":"p1481"}}},"toolbar_location":"below","left":[{"type":"object","name":"LinearAxis","id":"p1475","attributes":{"ticker":{"type":"object","name":"BasicTicker","id":"p1476","attributes":{"mantissas":[1,2,5]}},"formatter":{"type":"object","name":"BasicTickFormatter","id":"p1477"},"major_label_policy":{"type":"object","name":"AllLabels","id":"p1478"}}}],"below":[{"type":"object","name":"LinearAxis","id":"p1470","attributes":{"ticker":{"type":"object","name":"BasicTicker","id":"p1471","attributes":{"mantissas":[1,2,5]}},"formatter":{"type":"object","name":"BasicTickFormatter","id":"p1472"},"major_label_policy":{"type":"object","name":"AllLabels","id":"p1473"}}}],"center":[{"type":"object","name":"Grid","id":"p1474","attributes":{"axis":{"id":"p1470"}}},{"type":"object","name":"Grid","id":"p1479","attributes":{"dimension":1,"axis":{"id":"p1475"}}}],"background_fill_color":"#fcfcfc"}}]}}
    </script>
    <script type="text/javascript">
      (function() {
        const fn = function() {
          Bokeh.safely(function() {
            (function(root) {
              function embed_document(root) {
              const docs_json = document.getElementById('f6fce878-4576-4d23-b67e-6a2d068cfa5e').textContent;
              const render_items = [{"docid":"61dcb71e-5968-43b3-b6fb-e643ee640cbc","roots":{"p1458":"cf04c246-bc1f-4eb6-af65-b2c4ac6a347f"},"root_ids":["p1458"]}];
              root.Bokeh.embed.embed_items(docs_json, render_items);
              }
              if (root.Bokeh !== undefined) {
                embed_document(root);
              } else {
                let attempts = 0;
                const timer = setInterval(function(root) {
                  if (root.Bokeh !== undefined) {
                    clearInterval(timer);
                    embed_document(root);
                  } else {
                    attempts++;
                    if (attempts > 100) {
                      clearInterval(timer);
                      console.log("Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing");
                    }
                  }
                }, 10, root)
              }
            })(window);
          });
        };
        if (document.readyState != "loading") fn();
        else document.addEventListener("DOMContentLoaded", fn);
      })();
    </script>
  </body>
</html>